{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_utterance(data:json) -> json:\n",
    "    \"\"\"\n",
    "    Remove empty utterances from the data\n",
    "    \"\"\"\n",
    "    for example in data:\n",
    "        example['input']['conversation'] = [cvt for cvt in example['input']['conversation'] if cvt['utterance'] != '']\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# 이상치 output 처리\n",
    "def correct_wrong_output(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    1. Correct wrong speakers in outputs of train samples 'train-000401', 'train-000402, 'train-000111'\n",
    "    2. Add dot(.) at the end of the last sentence in outputs of train samples 'train-000130'\n",
    "    3. Replace speaker name 'SSD' with 'SD' in outputso of 'train-000030', 'train-000193' and 'dev-000085'\n",
    "    4. Remove duplicate sentences in outputs of dev samples 'dev-000093'.\n",
    "    5. Change '말했습니다,' to '말했습니다.' in outputs of train samples 'train-000044'\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # Correct wrong speakers\n",
    "        data[400]['output'] = data[400]['output'].replace('SD2100504','SD2110504')\n",
    "        data[401]['output'] = data[401]['output'].replace('SD2110503','SD2100503')\n",
    "        data[110]['output'] = data[110]['output'].replace('SD20010813','SD2001083')\n",
    "        # Add dot(.) at the end of the last sentence\n",
    "        data[129]['output'] = data[129]['output'] + '.'\n",
    "        # Replace speaker name\n",
    "        data[29]['output'] = data[29]['output'].replace('SSD', 'SD')\n",
    "        data[192]['output'] = data[192]['output'].replace('SSD', 'SD')\n",
    "        # Change '말했습니다,' to '말했습니다.'\n",
    "        data[43]['output'] = data[43]['output'].replace('말했습니다,','말했습니다.')\n",
    "\n",
    "\n",
    "    elif 'dev' in path:\n",
    "        # Replace speaker name\n",
    "        data[84]['output'] = data[84]['output'].replace('SSD', 'SD')\n",
    "        # Remove duplicate sentences\n",
    "        data[92]['output'] = '.'.join(data[92]['output'].split('.')[1:]).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# total summary(output의 맨 첫 번째 문장) 형식 통일을 위한 이상치 output 처리\n",
    "def change_weird_output(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Standardize the type of the output of train-000032, train-000418, dev-000074, dev-000093\n",
    "    \"\"\"\n",
    "    # Standardize the type of outputs\n",
    "    if 'train' in path:\n",
    "        # train-000032 : total_summary 교체\n",
    "        output = data[31]['output'].split('.')\n",
    "        total_summary = \"두 화자는 이 대화에서 진로 관련 고민에 대해 이야기했습니다. \"\n",
    "        data[31]['output'] = total_summary + '.'.join(output[1:])\n",
    "\n",
    "        # train-000418 : total_summary 추가\n",
    "        total_summary = \"두 화자는 이 대화에서 다이어트에 대해 이야기했습니다. \"\n",
    "        data[417]['output'] = total_summary + data[417]['output']\n",
    "\n",
    "    elif 'dev' in path:\n",
    "        # dev-000074 : total_summary 수정\n",
    "        data[73]['output'] = \"두 화자는 \"+ data[73]['output'] # 이 대화에서 -> 두 화자는 이 대화에서\n",
    "\n",
    "        # dev-000093 : total_summary 추가\n",
    "        total_summary = \"두 화자는 이 대화에서 엔시티와 방탄소년단에 대해 이야기 했습니다. \"\n",
    "        data[92]['output'] = total_summary + data[92]['output']\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# output에 SD가 예외적으로 들어간 경우 처리\n",
    "def remove_sd_in_total_summary(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove 'SD' in total_summary of train-000020 and train-000176\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # train-000020 : total_summary 수정\n",
    "        data[19]['output'] = data[19]['output'].replace('SD2000039의 꿈인 ','')\n",
    "\n",
    "        # train-000176 : total_summary '.' 가 빠져있던 것을 수정\n",
    "        output = data[175]['output']\n",
    "        data[175]['output'] = re.sub(r'(장단점에 대해 말했습니다)\\s+(SD\\d{7}(?:은|는))', r'\\1. \\2', output)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# utterance와 output에서는 '.' 뒤에 공백이 무조건 존재하는 형태로 통일 / 문장 맨 마지막의 경우는 '.'으로 통일\n",
    "def add_space_after_period_and_remove_control_characters(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Add space after period if there is no space after period\n",
    "    text = re.sub(r'\\.(?=\\S)', '. ', text)\n",
    "    \"\"\"\n",
    "    # Add space after period in utterances\n",
    "    for example in data:\n",
    "        example['input']['conversation'] = [{'speaker': cvt['speaker'], 'utterance': re.sub(r'\\.(?=\\S)', '. ', cvt['utterance']).strip()} for cvt in example['input']['conversation']]\n",
    "\n",
    "    if 'train' or 'dev' in path:\n",
    "        # Remove_control_characters and Add space after period in outputs\n",
    "        for example in data:\n",
    "            output = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', example['output'])\n",
    "            example['output'] = re.sub(r'\\.(?=\\S)', '. ', output).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# total summary(output의 맨 첫 번째 문장) 형식을 \"두 화자는 이 대화에서\"로 통일\n",
    "def total_summary_generalization(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Standardize the format of the total summary in the first sentence of the output \n",
    "    to start with \"두 화자는 이 대화에서\".\n",
    "    \"\"\"\n",
    "    types = [\"두 화자는\", \"화자들은\" ,\"두 사람은\", \"이 대화에서는\"] # \"두 화자는 이 대화에서\"\n",
    "    types2 = r\"SD\\d{7}(?:와|과).*SD\\d{7}(?:은|는)\"\n",
    "\n",
    "    if 'train' or 'dev' in path:\n",
    "        for example in data:\n",
    "            output = example['output']\n",
    "            total_summary = output.split('.')[0]\n",
    "\n",
    "            if \"두 화자는 이 대화에서\" in total_summary:\n",
    "                continue\n",
    "            elif re.search(types2, total_summary):\n",
    "                total_summary = re.sub(r'(.*)'+types2, '두 화자는 이 대화에서', total_summary)+'.'\n",
    "                example['output'] = total_summary+'.'.join(output.split('.')[1:])\n",
    "            else:\n",
    "                for type in types:\n",
    "                    if type in total_summary:\n",
    "                        total_summary = re.sub(r'(.*)'+type, '두 화자는 이 대화에서', total_summary)+'.'\n",
    "                        example['output'] = total_summary+'.'.join(output.split('.')[1:])\n",
    "                        break\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# output의 형식 통일 후, 중복 단어 제거\n",
    "def remove_duplicate_output_words(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove duplicate words in outputs of train samples \n",
    "    (그리고 그리고) 'train-000387', \n",
    "    (대화에서 대화에서) 'train-000383', 'train-000451', 'train-000479', 'train-000495'\n",
    "    (좋은 좋은) 'train-000268'\n",
    "    (화자 화자) 'train-000092', 'train-000231'\n",
    "    (할머니가 할머니가) 'train-000128'\n",
    "    (가도 가도) 'train-000338'\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # Remove duplicate words\n",
    "        ids = [387, 383, 451, 479, 495, 268, 92, 231, 128, 338]\n",
    "        for id in ids:\n",
    "            output = data[id-1]['output']\n",
    "            output = re.sub(r'\\b(\\w+)\\b(?:\\s+\\1\\b)+', r'\\1', output)\n",
    "            data[id-1]['output'] = output\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# stopword로 제거하기 전, 예외적인 경우 처리\n",
    "def remove_stopwords_exception(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    manual exception handling for removing stopwords in utterances\n",
    "    (\" 좋 \") : train과 dev에서는 의미없게 단어 사이에 추가된 단어이지만, test에서는 의미있는 단어로 사용되는 경우(좋 은데, 좋 을 것)가 있음\n",
    "        ex) 'test-000119' : \"좋 은데\" -> \"좋은데\"\n",
    "            'test-000303' : \"좋 을 것\" -> \"좋을 것\"\n",
    "            'test-000348' : \"좋 다고\" -> \"좋다고\"\n",
    "    \"\"\"\n",
    "    if 'test' in path:\n",
    "        # \" 좋 \" -> \" 좋\"\n",
    "        data[118]['input']['conversation'][-1]['utterance'] = data[118]['input']['conversation'][-1]['utterance'].replace(' 좋 ', ' 좋')\n",
    "        data[302]['input']['conversation'][-2]['utterance'] = data[302]['input']['conversation'][-2]['utterance'].replace(' 좋 ', ' 좋')\n",
    "        data[347]['input']['conversation'][4]['utterance'] = data[347]['input']['conversation'][4]['utterance'].replace(' 좋 ', ' 좋')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# SD\\d{7} 앞에 '화자' 제거\n",
    "def remove_hwaja_before_speaker_in_output(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove '화자' before 'SD\\d{7}' in outputs of train samples\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        for example in data:\n",
    "            output = example['output']\n",
    "            output = re.sub(r'화자\\s*(SD\\d{7})', r'\\1', output)\n",
    "            example['output'] = output\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# SD\\d{7} 뒤에 아무런 조사가 붙지 않은 경우 수정\n",
    "def add_josa_after_speaker_in_output(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    <Train>\n",
    "    - train-243 : SD2002060 또한 -> SD2002060도\n",
    "    - train-410 : 또 SD2100516 자신은 -> 또 자신은\n",
    "    - train-441 :  SD2110545 유기견을 -> 또 유기견을 / 또 SD2100546은 -> SD2100546은\n",
    "    - train-495 :  SD2100589에도 -> SD2100589에게도 / SD2100589 헬스장 -> SD2100589에게 헬스장\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        data[242]['output'] = data[242]['output'].replace('SD2002060 또한', 'SD2002060도')\n",
    "        data[409]['output'] = data[409]['output'].replace('또 SD2100516 자신은', '또 자신은')\n",
    "        data[440]['output'] = data[440]['output'].replace('SD2110545 유기견을', '또 유기견을').replace('또 SD2100546은', 'SD2100546은')\n",
    "        data[494]['output'] = data[494]['output'].replace('SD2100589에도', 'SD2100589에게도').replace('SD2100589 헬스장', 'SD2100589에게 헬스장')\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# speaker output 형식 통일\n",
    "def speaker_summary_generalization(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Standardize the format of the speaker summary in the first sentence of the output \n",
    "    to start with \"SD\\d{7}은(는)\".\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # exception handling \n",
    "        # train-000496 \"SD2100589가\" -> \"SD2100589는\"\n",
    "        # train-000476 \"SD2100573도\" -> \"SD2100573은\"\n",
    "        data[495]['output'] = data[495]['output'].replace('SD2100589가', 'SD2100589는')\n",
    "        data[475]['output'] = data[475]['output'].replace('SD2100573도', 'SD2100573은')\n",
    "    \n",
    "\n",
    "    def check_first_speaker_and_first_summary_speaker_is_same(example:json) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the first speaker and the first speaker summary speaker are the same.\n",
    "        \"\"\"\n",
    "        first_speaker = example['input']['conversation'][0]['speaker']\n",
    "        first_summary_speaker = re.search(r'SD\\d{7}', example['output']).group()\n",
    "        return first_speaker == first_summary_speaker\n",
    "\n",
    "    def make_speaker_summaries_bullet_point_format(text:str) -> str:\n",
    "        \"\"\"\n",
    "        Make the speaker summaries in bullet point format.\n",
    "        \"\"\"\n",
    "        output = \"\\n\".join([f\"- {sentence.strip()}. \" for sentence in text.split('.') if sentence.strip()])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def find_split_indexes(text: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Find the indexes(strat, end) to split the structured summary.\n",
    "        \"\"\"\n",
    "        # The number of 'SD{7}[은는]{1}'\n",
    "        num_speakers = len(re.findall(r'SD\\d{7}[은는]{1}', text))\n",
    "\n",
    "        # Split the structured summary based on the number of 'SD{7}[은는]{1}'\n",
    "        if num_speakers == 2: \n",
    "            mathes = re.finditer(r'SD\\d{7}[은는]{1}', text)\n",
    "            return [(match.group(), match.start()) for match in mathes] # [(speaker1, start_id_1), (speaker2, start_id_2)]\n",
    "        \n",
    "        elif num_speakers in [0, 1]:\n",
    "            matches = re.finditer(r'SD\\d{7}\\w+', text)\n",
    "\n",
    "            first_match = next(matches)\n",
    "            first_tuple = (first_match.start(), first_match.group())\n",
    "\n",
    "            for match in matches:\n",
    "                if match.group()[:9] == first_tuple[1][:9]: # SD{7}가 같은 경우\n",
    "                    continue\n",
    "                return [(first_tuple[1], first_tuple[0]), (match.group(), match.start())]\n",
    "            \n",
    "        elif num_speakers >= 3:\n",
    "            matches = re.finditer(r'SD\\d{7}[은는]{1}', text)\n",
    "\n",
    "            first_match = next(matches)\n",
    "            first_tuple = (first_match.start(), first_match.group())\n",
    "\n",
    "            for match in matches:\n",
    "                if match.group()[:9] == first_tuple[1][:9]: # SD{7}가 같은 경우\n",
    "                    continue\n",
    "                return [(first_tuple[1], first_tuple[0]), (match.group(), match.start())]\n",
    "            \n",
    "\n",
    "    if 'test' in path:\n",
    "        for example in data:\n",
    "            # Find speaker_1 and speaker_2\n",
    "            speaker_1 = example['input']['conversation'][0]['speaker']\n",
    "\n",
    "            for speaker in example['input']['conversation']:\n",
    "                if speaker['speaker'] != speaker_1:\n",
    "                    speaker_2 = speaker['speaker']\n",
    "                    break\n",
    "                \n",
    "            example['input']['speaker_1'] = speaker_1\n",
    "            example['input']['speaker_2'] = speaker_2\n",
    "\n",
    "    elif 'train' or 'dev' in path:\n",
    "        for example in data:\n",
    "            output = example['output']\n",
    "\n",
    "            # Find the indexes to split the structured summary\n",
    "            split_indexes = find_split_indexes(output) # [(r'speaker1\\w+', start_id_1), (r'speaker2\\w+', start_id_2)]\n",
    "            speaker_1, speaker_2 = split_indexes[0][0][:9], split_indexes[1][0][:9] # SD{7}\n",
    "\n",
    "            # Split the structured summary\n",
    "            total_summary = output[:split_indexes[0][1]].strip()\n",
    "            if check_first_speaker_and_first_summary_speaker_is_same(example):\n",
    "                # The first speaker and the first speaker summary speaker are the same\n",
    "                example['input']['speaker_1'] = speaker_1\n",
    "                example['input']['speaker_2'] = speaker_2\n",
    "\n",
    "                speaker_1_summary = output[split_indexes[0][1]:split_indexes[1][1]].strip()\n",
    "                speaker_2_summary = output[split_indexes[1][1]:].strip()\n",
    "            else:\n",
    "                # The first speaker and the first speaker summary speaker are different\n",
    "                speaker_1, speaker_2 = speaker_2, speaker_1 # Swap the speakers\n",
    "                example['input']['speaker_1'] = speaker_1\n",
    "                example['input']['speaker_2'] = speaker_2\n",
    "\n",
    "                speaker_1_summary = output[split_indexes[1][1]:].strip()\n",
    "                speaker_2_summary = output[split_indexes[0][1]:split_indexes[1][1]].strip()\n",
    "\n",
    "\n",
    "            # speaker_1_summary = make_speaker_summaries_bullet_point_format(speaker_1_summary)\n",
    "            # speaker_2_summary = make_speaker_summaries_bullet_point_format(speaker_2_summary)\n",
    "\n",
    "            # Standardize the format of the speaker summary\n",
    "            output_format = f'''## 전반적인 요약\\n{total_summary}\\n\\n## {speaker_1} 요약\\n{speaker_1_summary}\\n\\n## {speaker_2} 요약\\n{speaker_2_summary}'''\n",
    "            \n",
    "            example['output'] = output_format\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# subject_keyword 반복 단어 제거\n",
    "def remove_duplicate_subject_keywords(data:json, path:str) -> json:\n",
    "    '''\n",
    "    Remove duplicate words in subject_keywords of dev samples 'dev-000045', 'dev-000086', 'dev-000087', 'dev-000088', 'dev-000089'\n",
    "    '''\n",
    "    if 'dev' in path:\n",
    "        # Remove duplicate words\n",
    "        ids = [44, 85, 86, 87, 88]\n",
    "        for id in ids:\n",
    "            subject_keywords = data[id]['input']['subject_keyword']\n",
    "            subject_keywords = list(set(subject_keywords))\n",
    "            data[id]['input']['subject_keyword'] = subject_keywords\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# speaker1의 utterance 개수가 50개가 넘는 샘플 제거\n",
    "def remove_samples_with_more_than_50_utterances(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove the samples with more than 50 utterances in speaker1\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # Remove the samples with more than 50 utterances in speaker1\n",
    "        # indexes = [6, 310, 311, 323, 324, 339, 349, 358, 359, 362, 413, 444, 460, 461, 492] # from utterance_length_eda.ipynb\n",
    "        \n",
    "        indexes = [6, 310, 311, 323, 324, 339, 349, 358, 359, 362, 413, 444, 460, 461, 492]\n",
    "        data = [data[i] for i in range(len(data)) if i not in indexes]\n",
    "\n",
    "    return data \n",
    "\n",
    "\n",
    "# 반복되는 단어 조합 제거\n",
    "def make_one_repeated_words(data:json, path:str, iter:int=0) -> json:\n",
    "    \"\"\"\n",
    "    Replace the repeated words in the text with one word.\n",
    "\n",
    "    Parameters:\n",
    "    data (json): Data to be processed.\n",
    "    path (str): Path to save the processed data.\n",
    "\n",
    "    Returns:\n",
    "    data (json): Processed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Function for removing repeated words\n",
    "    def removeing_repeated_words(data:json, repeated_phrase_indices:dict, mode:str) -> json:\n",
    "    # repeated_phrase_indices = {key : index, value : repeated phrase}\n",
    "\n",
    "        for idx in tqdm(repeated_phrase_indices.keys(), total=len(repeated_phrase_indices), desc=f'Removing repeated phrases in {mode} data ... (Phase {iter})'):\n",
    "            repeated_phrases = repeated_phrase_indices[idx]\n",
    "            for phrase in repeated_phrases:\n",
    "                pattern = rf'\\b{phrase} {phrase}'\n",
    "                try:\n",
    "                    for i, turn in enumerate(data[idx]['input']['conversation']):\n",
    "                        if re.search(pattern, turn['utterance']):\n",
    "                            data[idx]['input']['conversation'][i]['utterance'] = re.sub(pattern, phrase, turn['utterance'])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Remove repeated words in the conversation\n",
    "    if 'train' in path:\n",
    "        with open(f'./src/data/train_repeated_phrase_indices_{iter}.pkl', 'rb') as file:\n",
    "            repeated_phrase_indices = pickle.load(file)\n",
    "        \n",
    "        removeing_repeated_words(data, repeated_phrase_indices, mode='train')\n",
    "\n",
    "    elif 'dev' in path:\n",
    "        with open(f'./src/data/dev_repeated_phrase_indices_{iter}.pkl', 'rb') as file:\n",
    "            repeated_phrase_indices = pickle.load(file)\n",
    "        removeing_repeated_words(data, repeated_phrase_indices, mode='dev')\n",
    "\n",
    "    elif 'test' in path:\n",
    "        with open(f'./src/data/test_repeated_phrase_indices_{iter}.pkl', 'rb') as file:\n",
    "            repeated_phrase_indices = pickle.load(file)\n",
    "        removeing_repeated_words(data, repeated_phrase_indices, mode='test')\n",
    "\n",
    "    return data\n",
    "\n",
    "# 좀 삭제\n",
    "def remove_jom(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove '좀' in utterances and outputs\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove '좀' in utterances and outputs\n",
    "    for example in data:\n",
    "        example['output'] = re.sub(r'\\b좀\\b', '', example['output'])\n",
    "        # '  ' -> ' '\n",
    "        example['output'] = re.sub(r'\\s+', ' ', example['output']).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# 전반적인 요약 문장이 2개인 경우(506개 중 4개) 두 번째 요약 문장을 제거\n",
    "def remove_second_summary(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove the second summary in the output if there are two summaries in the output\n",
    "    \"\"\"\n",
    "\n",
    "    indexes = [59, 63, 261, 475]\n",
    "    \n",
    "    if 'train' in path:\n",
    "        for idx in indexes:\n",
    "            output = data[idx]['output']\n",
    "            output = output.split('.')\n",
    "            data[idx]['output'] = output[0] + '.' + '.'.join(output[2:])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# 이어지는 다음 턴 속 반복 문장 제거\n",
    "def remove_repeated_sentences_in_next_turn(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove repeated sentences in the conversation.\n",
    "    [train]\n",
    "    {16: ['turn : 1   (current ✓)  prev: 앞으로 먹어 보고 싶은 있 먹 | current: 앞으로 먹어 보고 싶은 먹거리가 있나요?'], \n",
    "    162: ['turn : 3   (current ✓)  prev: 그러니까 결혼 생활 중에서 가장 행복했었었던 때 | current: 결혼 생활 중에서 가장 행복했던 때'],\n",
    "    169: ['turn : 9   (prev ✓)  prev:  이게 뭔가 이게 나중에 어르신들 했을 때 이게 참 이게 뭐가 잘되게 이게 융합 맞은 맞을 거 같아 | current: 이게 뭐가 잘되게 이게 융합 맞은 맞을 거 같아'],\n",
    "    278: ['turn : 19  (current ✓)   prev: 안 먹으면 이제는 힘이 없으니까 말이 안 나올 거 같고 그냥 | current: 안 먹으면 이제는 힘이 없으니까 말이 안 나올 거 같고 그냥 적게 먹는 게 다이어트 하는 방법인 거 같아요'],\n",
    "    358: ['turn : 64  (current ✓)  prev: 직장에서? | current: 오빠 직장에서?'],\n",
    "    381: ['turn : 10  (prev ✓)   prev: 내나 그런 느낌이지 않아? | current: 그런 느낌이지'],\n",
    "    383: ['turn : 2   (current ✓)  prev: 백두산? | current: 갑자기 백두산?'],\n",
    "    505: ['turn : 17  (prev ✓)   prev: company-name3 집은 아직 열고 있긴 한데 건너편에 재개발이 되다 보니까 상권들이 다 안 좋아져서 많이들 맛집들이 문을 닫으려고 하는 거 같아 | current: 아직 열고 있긴 한데 건너편에 재개발이 되다 보니까 상권들이 다 안 좋아져서 많이들 맛집들이 문을 닫으려고 하는 거 같아']}\n",
    "    \n",
    "    [test]\n",
    "    {198: ['turn : 16 (prev ✓)   prev: 정말로 뭔가 평이하게 큰소리 한번 나지 않고 그렇게 그런 환경에서 자랄 수 있던 것이 정말로 감사하고 컸다라는 것을 알아가게 되는 것 같습니다 | current: 정말로 뭔가 평이하게 큰소리 한번 나지 않고 그렇게 그런 환경에서 자랄 수 있던 것이 정말로 감사하고 컸다라는 것을 알아가게 되는 것 같습니다'],\n",
    "    218: ['turn : 15  (current ✓)   prev:  제목이 | current: 제목이 뭐야?'],\n",
    "    331: ['turn : 3   (current ✓)  prev:  진짜 습하면은 | current: 습하면은 진짜 아무것도 못하겠는 거예요'],\n",
    "    372: ['turn : 3   (prev ✓)  prev:  당연히 직접 먹는 걸 좋아합니다 | current: 먹는 걸 좋아합니다']}\n",
    "    \"\"\"\n",
    "    train_indexes_and_turns = [(16,1,'current'), \n",
    "                             (162,3,'current'),\n",
    "                             (169,9, 'prev'),\n",
    "                             (278,19,'current'),\n",
    "                             (358,64,'current'),\n",
    "                             (381,10,'prev'),\n",
    "                             (383,2,'current'),\n",
    "                             (505,17,'prev')]\n",
    "    \n",
    "    test_indexes_and_turns = [(198,16,'prev'),\n",
    "                            (218,15,'current'),\n",
    "                            (331,3,'current'),\n",
    "                            (372,3,'prev')]\n",
    "\n",
    "    def change_sent(data, indexes_and_turns):\n",
    "        for idx, turn, mode in indexes_and_turns:\n",
    "            if mode == 'prev':\n",
    "                # Remain the previous turn and remove the current turn's first sentence\n",
    "                current_utterance = data[idx]['input']['conversation'][turn]['utterance']\n",
    "                if '.' in current_utterance:\n",
    "                    data[idx]['input']['conversation'][turn]['utterance'] = re.sub(r'^[^.]*\\.', '', data[idx]['input']['conversation'][turn]['utterance'])\n",
    "                else:\n",
    "                    data[idx]['input']['conversation'][turn]['utterance'] = ''\n",
    "            elif mode == 'current':\n",
    "                # Remain the current turn and remove the previous turn's last sentence\n",
    "                prev_utterance = data[idx]['input']['conversation'][turn-1]['utterance']\n",
    "                if '.' in prev_utterance:\n",
    "                    data[idx]['input']['conversation'][turn-1]['utterance'] = '.'.join(prev_utterance.split('.')[:-2]) + '.'\n",
    "                else:\n",
    "                    data[idx]['input']['conversation'][turn-1]['utterance'] = ''\n",
    "        \n",
    "        return data\n",
    "\n",
    "    if 'train' in path:\n",
    "        data = change_sent(data, train_indexes_and_turns)\n",
    "        \n",
    "    elif 'test' in path:\n",
    "        data = change_sent(data, test_indexes_and_turns)\n",
    "\n",
    "    return data \n",
    "    \n",
    "    \n",
    "# 의미 없는 \" 예.\", \" 네.\", \" 응.\" 제거\n",
    "def remove_useless_word(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Remove the meaningless words in the conversation. such as \" 예.\", \" 네.\", \" 응.\" -> \" \"\n",
    "    \"\"\"\n",
    "    # Remove the meaningless words in the conversation\n",
    "\n",
    "    if 'train' or 'dev' or 'test' in path:\n",
    "        for example in data:\n",
    "            for i, turn in enumerate(example['input']['conversation']):\n",
    "                if re.search(r' (예|네|응)\\.\\s*', turn['utterance']):\n",
    "                    example['input']['conversation'][i]['utterance'] = re.sub(r' (예|네|응)\\.\\s*', ' ', turn['utterance'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# name 토큰 전처리\n",
    "def name_token_preprocessing(data:json, path:str) -> json:\n",
    "    \"\"\"\n",
    "    Preprocess the name tokens in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exception handling for name tokens\n",
    "    # dev-000078 :'name3 씨와 name2 씨를' -> 'name3와 name2를'\n",
    "    if 'dev' in path:\n",
    "        data[77]['input']['conversation'][10]['utterance'] = data[77]['input']['conversation'][10]['utterance'].replace('name3 씨와 name2 씨를', 'name3와 name2를')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# name 조사 처리\n",
    "def replace_name_josa_in_conversation(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Replace the name josa in the conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exception handling for name tokens\n",
    "    # train\n",
    "    #    - 20, 381, 425, 424, 425, 478 : name{는} 제거\n",
    "    #    - 484, 485, 147 : name{} 이는 -> 님은\n",
    "    #    - 90 : 맨 마지막 turn 제거 \"그래서 name 이 딱 네가 name 이의 배잖아. 너의 가격의 반이 name 이의 노트북 가격이잖아.\"\n",
    "\n",
    "    # name 언니 처리 \n",
    "    # train \n",
    "    #     - 67, 68 : name2 언니 -> 언니\n",
    "    #     - 90 : name1 언니 -> 언니\n",
    "    #     - 96, 97 : name1 언니 -> 언니\n",
    "\n",
    "    # dev\n",
    "    #   -77 : name3 씨와 name2 씨를' -> 'name3와 name2를'\n",
    "    if 'train' in path:\n",
    "        for i in 20, 381, 425, 424, 425, 478:\n",
    "            for turn in data[i]['input']['conversation']:\n",
    "                turn['utterance'] = re.sub(r'\\bname\\d*\\w*\\b', '', turn['utterance'])\n",
    "        for i in 484, 485, 147:\n",
    "            for turn in data[i]['input']['conversation']:\n",
    "                turn['utterance'] = re.sub(r'\\bname\\d*\\s*(\\w+)', '님은', turn['utterance'])\n",
    "        data[90]['input']['conversation'] = data[90]['input']['conversation'][:-1]\n",
    "\n",
    "        for i in 67, 68, 90, 96, 97:\n",
    "            for turn in data[i]['input']['conversation']:\n",
    "                turn['utterance'] = re.sub(r'\\bname\\d*\\s*(언니)', '언니', turn['utterance'])\n",
    "    \n",
    "    if 'dev' in path:\n",
    "        data[77]['input']['conversation'][10]['utterance'] = data[77]['input']['conversation'][10]['utterance'].replace('name3 씨와 name2 씨를', 'name3와 name2를')\n",
    "\n",
    "\n",
    "    relace_name_dict = {\"씨\" : \"님\", \"씨가\" : \"님이\", \"씨께서는\" : \"님은\", \"씨께서도\" : \"님도\", \"씨는\" : \"님은\", \"씨는요\" : \"님은요\", \"씨도\" : \"님도\", \"씨랑\" : \"님이랑\", \n",
    "                    \"씨부터\" : \"님부터\", \"씨와\" : \"님과\", \"씨와는\" : \"님과는\", \"씨처럼\" : \"님처럼\", \"씨한테\" : \"님한테\",\n",
    "                    \"님\" : \"님\", \"님과\" : \"님과\", \"님께서\" : \"님은\", \"님는\" : \"님은\", \"님께서는\" : \"님은\", \"님께서도\" : \"님도\", \"님도\" : \"님도\", \n",
    "                    \"님만의\" : \"님만의\", \"님은\" : \"님은\", \"님의\" : \"님의\", \"님이\" : \"님이\", \"님이랑\" : \"님이랑\", \"님처럼\" : \"님처럼\", \"님하고\" : \"님하고\",\n",
    "                    \"형님은\" : \"형님은\", \"실장님은\" : \"실장님은\", \"실장님도\" : \"실장님도\", \"형님께서는\" : \"형님께서는\"}\n",
    "    \n",
    "    def replace_with_dict(match):\n",
    "        key = match.group(1)\n",
    "        return relace_name_dict.get(key, match.group(0))\n",
    "\n",
    "    if 'train' or 'dev' or 'test' in path:\n",
    "        for example in data:\n",
    "            for turn in example['input']['conversation']:\n",
    "                turn['utterance'] = re.sub(r'\\bname\\d*\\s*(\\w+)', replace_with_dict , turn['utterance'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stopwords_pattern = stopwords_pattern = [r'\\w~', r'\\b으\\b', r'\\b그\\b', r'\\b뭐\\b', r'\\b어\\b',  r'\\b인제\\b', r'\\b이제\\b', r'\\b막\\b', r'\\b아\\b', r'\\b음\\b', r'\\b읍\\b', r'\\b오\\b', \n",
    "    r'\\b으\\b', r'좋 ', r'\\b크\\b', r'\\b스\\b', r'\\. \\.', r'^\\s*\\.\\s{1}',r'\\b하\\b', r'\\b예\\b']#, r'\\b좀\\b'] # r'name[0-9]\\S*'\n",
    "\n",
    "    # 커스텀 불용어 제거\n",
    "    for pattern in stopwords_pattern:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # x를 포함한 단어 제거\n",
    "    text = re.sub(r'\\b[가-힣a-zA-Z]*[xX][가-힣a-zA-Z]*\\b', '', text)\n",
    "\n",
    "    # 단어가 두 번 이상 반복되는 경우 -> 1개로\n",
    "    # text = re.sub(r'\\b(\\w)\\s+\\1\\b', r'\\1', text)\n",
    "    # text = re.sub(r'\\b([가-힣a-zA-Z0-9_]+)\\s+\\1\\b', r'\\1', text)\n",
    "    text = re.sub(r'\\b(\\w+)\\b(?:\\s+\\1\\b)+', r'\\1', text)\n",
    "\n",
    "    # 공백 두 번 이상 연속 -> 1개로\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # 간단한 후처리\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess(path:str):\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # remove_stopwords_exception\n",
    "    data = remove_stopwords_exception(data, path)\n",
    "\n",
    "    # correct_wrong_outputㅋ\n",
    "    data = correct_wrong_output(data, path)\n",
    "\n",
    "    # change_weird_output\n",
    "    data = change_weird_output(data, path)\n",
    "\n",
    "    # remove_sd_in_total_summary\n",
    "    data = remove_sd_in_total_summary(data, path)\n",
    "\n",
    "    # add_space_after_period and strip\n",
    "    data = add_space_after_period_and_remove_control_characters(data, path)\n",
    "    \n",
    "    # total_summary_generalization\n",
    "    data = total_summary_generalization(data, path)\n",
    "\n",
    "    # # remove_empty_utterance\n",
    "    # data = remove_empty_utterance(data)\n",
    "\n",
    "    # remove_duplicate_output_words\n",
    "    data = remove_duplicate_output_words(data, path)\n",
    "\n",
    "    # remove_hwaja_before_speaker_in_output\n",
    "    data = remove_hwaja_before_speaker_in_output(data, path)\n",
    "\n",
    "    # preprocess the dataset\n",
    "    for example in data:\n",
    "        for cvt in example['input']['conversation']:\n",
    "            cvt['utterance'] = remove_stopwords(cvt['utterance'])\n",
    "\n",
    "    # remove_empty_utterance\n",
    "    data = remove_empty_utterance(data)\n",
    "\n",
    "    # add_josa_after_speaker_in_output\n",
    "    data = add_josa_after_speaker_in_output(data, path)\n",
    "\n",
    "    # speaker_summary_generalization\n",
    "    data = speaker_summary_generalization(data, path)\n",
    "\n",
    "    # remove_duplicate_subject_keywords\n",
    "    data = remove_duplicate_subject_keywords(data, path)\n",
    "\n",
    "    # remove_samples_with_more_than_50_utterances\n",
    "    # data = remove_samples_with_more_than_50_utterances(data, path)\n",
    "\n",
    "    make_one_repeated_words_iter_0\n",
    "    data = make_one_repeated_words(data, path, iter=0)\n",
    "\n",
    "    # make_one_repeated_words_iter_1\n",
    "    data = make_one_repeated_words(data, path, iter=1)\n",
    "\n",
    "    # Save the preprocessed dataset\n",
    "    with open(path.split('/')[-1].split('_')[1].split('.')[0]+'.json', 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Preprocessing of {path} is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of ../resource/data/일상대화요약_train.json is done!\n"
     ]
    }
   ],
   "source": [
    "preprocess('../resource/data/일상대화요약_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of ../resource/data/일상대화요약_dev.json is done!\n"
     ]
    }
   ],
   "source": [
    "preprocess('../resource/data/일상대화요약_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of ../resource/data/일상대화요약_test.json is done!\n"
     ]
    }
   ],
   "source": [
    "preprocess('../resource/data/일상대화요약_test.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
